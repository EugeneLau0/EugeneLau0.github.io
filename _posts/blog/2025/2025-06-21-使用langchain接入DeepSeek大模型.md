---
title: ä½¿ç”¨LangChainæ¥å…¥DeepSeekå¤§æ¨¡å‹
categories:
  - Blog
tags:
  - AI
  - å¤§æ¨¡å‹
  - DeepSeek
  - OpenAI
  - LangChain
  - LLM
  - Jupyter Notebook
---

LangChainçš„é»˜è®¤æ¨¡å‹æ˜¯OpenAIï¼Œæœ¬ç¯‡å°†ä»‹ç»å¦‚ä½•ä½¿ç”¨LangChainæ”¯æŒDeepSeekçš„deepseek-chatæ¨¡å‹ã€‚

## LangChain

LangChainè¿™ä¸ªæ¡†æ¶æ˜¯ç”¨æ¥å¼€å‘å¤§æ¨¡å‹çš„åº”ç”¨ç¨‹åºï¼Œå¯ä»¥ç®€åŒ–LLMåº”ç”¨ç¨‹åºçš„æ„å»ºã€‚

ä¸­æ–‡ä»‹ç»å¯ä»¥å‚è€ƒè¿™ä¸ªç½‘ç«™ï¼Œhttps://python.langchain.ac.cn/docs/introduction/ ï¼Œ

å…¶å¯¹åº”çš„å®˜æ–¹ç½‘ç«™å°¾ï¼Œhttps://python.langchain.com/docs/introduction/ ã€‚

LangChainçš„æ¶æ„å›¾ï¼š

![](https://python.langchain.ac.cn/svg/langchain_stack_112024.svg)

## å®šä¹‰DeepSeekæ¨¡å‹

è¿™é‡Œé‡‡ç”¨APIæ¥å…¥çš„æ–¹å¼ï¼Œå°è£…ä¸€ä¸ªåº”ç”¨LangChainè°ƒç”¨DeepSeek LLM APIçš„å·¥å…·ã€‚

æºä»£ç å¦‚ä¸‹ï¼š

```python
from langchain_core.language_models.llms import BaseLLM
from langchain_core.outputs import LLMResult
from typing import Optional, List, Dict, Any, Iterator
import requests


class DeepSeekLLM(BaseLLM):
    """å°è£…DeepSeek LLM"""
    api_key: str  # DeepSeek API Key
    model: str = "deepseek-chat"  # é»˜è®¤æ¨¡å‹
    temperature: float = 0.7  # æ¸©åº¦å‚æ•°
    base_url: str = "https://api.deepseek.com/v1"  # DeepSeek API åœ°å€

    def _generate(
            self,
            prompts: List[str],
            stop: Optional[List[str]] = None,
            run_manager: Optional[Any] = None,
            **kwargs: Any,
    ) -> LLMResult:
        """å®ç°å¿…é¡»çš„æŠ½è±¡æ–¹æ³•"""
        responses = []
        for prompt in prompts:
            response = self._call(prompt, stop=stop, **kwargs)
            responses.append(response)
        return LLMResult(generations=[[{"text": r}] for r in responses])

    def _call(
            self,
            prompt: str,
            stop: Optional[List[str]] = None,
            **kwargs: Any,
    ) -> str:
        """è°ƒç”¨DeepSeek API"""
        url = f"{self.base_url}/chat/completions"
        headers = {"Authorization": f"Bearer {self.api_key}"}
        data = {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": self.temperature,
            **kwargs
        }
        if stop:
            data["stop"] = stop

        response = requests.post(url, json=data, headers=headers)
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]

    @property
    def _llm_type(self) -> str:
        return "deepseek"
```

åç»­åœ¨éœ€è¦ä½¿ç”¨çš„åœ°æ–¹ï¼Œè¿›è¡Œ`import`å³å¯ã€‚

```python
from ai.DeepSeekLLM import DeepSeekLLM
```

## ç›´æ¥è°ƒç”¨

æ‰§è¡Œä»¥ä¸‹ä»£ç ï¼Œå°†é€šè¿‡APIç›´æ¥è®¿é—®DeepSeekçš„chatæ¨¡å‹ã€‚

```python
from openai import OpenAI
# ä»envæ–‡ä»¶ä¸­åŠ è½½api_key
from dotenv import load_dotenv
import os
load_dotenv()
# print(os.getenv("deepseek_api_key"))
client = OpenAI(
    api_key=os.getenv("deepseek_api_key"),
    base_url="https://api.deepseek.com"
)

response = client.chat.completions.create(
    model="deepseek-chat",
    messages=[
        {"role": "system", "content": "who are you "},
        {"role": "user", "content": "Hello"},
    ],
    stream=False
)

print('æ¨¡å‹åˆ—è¡¨ï¼š' + str(client.models.list().data))

print('å¼€åœºç™½ï¼š' + response.choices[0].message.content)
```

é€šè¿‡è°ƒè¯•ï¼Œå¾—åˆ°æ¨¡å‹åˆ—è¡¨ä¸ºï¼š

```sh
æ¨¡å‹åˆ—è¡¨ï¼š[Model(id='deepseek-chat', created=None, object='model', owned_by='deepseek'), Model(id='deepseek-reasoner', created=None, object='model', owned_by='deepseek')]
```

é€šè¿‡Modelçš„idå¯ä»¥çœ‹åˆ°æœ‰2ä¸ªå¯ä»¥ä½¿ç”¨çš„æ¨¡å‹ï¼Œdeepseek-chatå’Œdeepseek-reasonerï¼Œå³å¯¹è¯æ¨¡å‹å’Œæ¨ç†æ¨¡å‹ã€‚

è¿”å›çš„å¼€åœºç™½ä¸ºï¼š

```sh
å¼€åœºç™½ï¼šHello! I'm an AI assistant here to help you with questions, information, or just to chat. How can I assist you today? ğŸ˜Š
```

å¯ä»¥çœ‹åˆ°ï¼Œå·²ç»å¯ä»¥æ­£å¸¸çš„è®¿é—®DeepSeekå¤§æ¨¡å‹å•¦ã€‚æ¥ä¸‹æ¥å°†è¿›ä¸€æ­¥å°è¯•å…¶ä»–çš„è°ƒç”¨æ–¹å¼ã€‚

## function calling æ¨¡å¼

å‡½æ•°è°ƒç”¨ï¼ˆFunction Callingï¼‰ æ˜¯ä¸€ç§è®©å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿä¸å¤–éƒ¨å·¥å…·äº¤äº’çš„å…³é”®æŠ€æœ¯ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒLLMå¯ä»¥æ‰©å±•å…¶èƒ½åŠ›ï¼Œä¾‹å¦‚è·å–å®æ—¶æ•°æ®æˆ–æ‰§è¡Œç‰¹å®šä»»åŠ¡ã€‚å‡½æ•°è°ƒç”¨çš„æ ¸å¿ƒåœ¨äºï¼ŒLLMæ ¹æ®ç”¨æˆ·è¾“å…¥ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨æŸä¸ªå‡½æ•°ï¼Œå¹¶ä»¥ç»“æ„åŒ–çš„æ–¹å¼è¾“å‡ºè°ƒç”¨ä¿¡æ¯ã€‚

ä»£ç ç¤ºä¾‹ï¼š

```python
from openai import OpenAI
from dotenv import load_dotenv
import os

load_dotenv()

client = OpenAI(api_key=os.getenv("deepseek_api_key"), base_url="https://api.deepseek.com")

def send_messages(messages):
    response = client.chat.completions.create(
        model="deepseek-chat",
        messages=messages,
        tools=tools
    )
    return response.choices[0].message


tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Get weather of an location, the user shoud supply a location first",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "The city and state, e.g. San Francisco, CA",
                    }
                },
                "required": ["location"]
            },
        }
    },
]

messages = [{"role": "user", "content": "How's the weather in Hangzhou?"}]
message = send_messages(messages)
print(f"User>\t {messages[0]['content']}")

tool = message.tool_calls[0]
messages.append(message)

messages.append({"role": "tool", "tool_call_id": tool.id, "content": "25â„ƒ"})
message = send_messages(messages)
print(f"Model>\t {message.content}")
```

ä¸Šé¢çš„ä»£ç ä¸­ï¼Œå®šä¹‰äº†ä¸€ä¸ªè·å–å¤©æ°”çš„å‡½æ•°ï¼ˆget_weatherï¼‰ï¼Œé€šè¿‡descriptionå­—æ®µæè¿°å¤§æ¨¡å‹åº”è¯¥å¦‚ä½•å“åº”ï¼Œå¹¶ä¸”å¢åŠ äº†parametersæŒ‡å®šè¿”å›çš„æ•°æ®æ ¼å¼ã€‚

è·‘èµ·æ¥çœ‹çœ‹ï¼š

```sh
User>	 How's the weather in Hangzhou?
Model>	 The current weather in Hangzhou is 25Â°C (77Â°F). It's a pleasant temperature - not too hot and not too cold.
```

é€šè¿‡function callingæ¨¡å¼ï¼Œå¯ä»¥ä»å¤§æ¨¡å‹ä¸­å¾—åˆ°æ›´åŠ ç²¾å‡†çš„ç»“æœï¼Œé¿å…é‚£äº›é•¿ç¯‡å¤§è®ºã€‚

