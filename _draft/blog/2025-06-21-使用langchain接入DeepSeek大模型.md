---
title: ä½¿ç”¨LangChainæ¥å…¥DeepSeekå¤§æ¨¡å‹
categories:
  - Blog
tags:
  - AI
  - å¤§æ¨¡å‹
  - DeepSeek
  - OpenAI
  - LangChain
  - LLM
  - Jupyter Notebook
---

LangChainçš„é»˜è®¤æ¨¡å‹æ˜¯OpenAIï¼Œæœ¬ç¯‡å°†ä»‹ç»å¦‚ä½•ä½¿ç”¨LangChainæ”¯æŒDeepSeekçš„deepseek-chatæ¨¡å‹ã€‚

## LangChain

LangChainè¿™ä¸ªæ¡†æ¶æ˜¯ç”¨æ¥å¼€å‘å¤§æ¨¡å‹çš„åº”ç”¨ç¨‹åºï¼Œå¯ä»¥ç®€åŒ–LLMåº”ç”¨ç¨‹åºçš„æ„å»ºã€‚

ä¸­æ–‡ä»‹ç»å¯ä»¥å‚è€ƒè¿™ä¸ªç½‘ç«™ï¼Œhttps://python.langchain.ac.cn/docs/introduction/ ï¼Œ

å…¶å¯¹åº”çš„å®˜æ–¹ç½‘ç«™å°¾ï¼Œhttps://python.langchain.com/docs/introduction/ ã€‚

LangChainçš„æ¶æ„å›¾ï¼š

![](https://python.langchain.ac.cn/svg/langchain_stack_112024.svg)

## å®šä¹‰DeepSeekæ¨¡å‹

è¿™é‡Œé‡‡ç”¨APIæ¥å…¥çš„æ–¹å¼ï¼Œå°è£…ä¸€ä¸ªåº”ç”¨LangChainè°ƒç”¨DeepSeek LLM APIçš„å·¥å…·ã€‚

æºä»£ç å¦‚ä¸‹ï¼š

```python
from langchain_core.language_models.llms import BaseLLM
from langchain_core.outputs import LLMResult
from typing import Optional, List, Dict, Any, Iterator
import requests


class DeepSeekLLM(BaseLLM):
    """å°è£…DeepSeek LLM"""
    api_key: str  # DeepSeek API Key
    model: str = "deepseek-chat"  # é»˜è®¤æ¨¡å‹
    temperature: float = 0.7  # æ¸©åº¦å‚æ•°
    base_url: str = "https://api.deepseek.com/v1"  # DeepSeek API åœ°å€

    def _generate(
            self,
            prompts: List[str],
            stop: Optional[List[str]] = None,
            run_manager: Optional[Any] = None,
            **kwargs: Any,
    ) -> LLMResult:
        """å®ç°å¿…é¡»çš„æŠ½è±¡æ–¹æ³•"""
        responses = []
        for prompt in prompts:
            response = self._call(prompt, stop=stop, **kwargs)
            responses.append(response)
        return LLMResult(generations=[[{"text": r}] for r in responses])

    def _call(
            self,
            prompt: str,
            stop: Optional[List[str]] = None,
            **kwargs: Any,
    ) -> str:
        """è°ƒç”¨DeepSeek API"""
        url = f"{self.base_url}/chat/completions"
        headers = {"Authorization": f"Bearer {self.api_key}"}
        data = {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": self.temperature,
            **kwargs
        }
        if stop:
            data["stop"] = stop

        response = requests.post(url, json=data, headers=headers)
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]

    @property
    def _llm_type(self) -> str:
        return "deepseek"
```

åç»­åœ¨éœ€è¦ä½¿ç”¨çš„åœ°æ–¹ï¼Œè¿›è¡Œ`import`å³å¯ã€‚

```python
from ai.DeepSeekLLM import DeepSeekLLM
```

## ç›´æ¥è°ƒç”¨

æ‰§è¡Œä»¥ä¸‹ä»£ç ï¼Œå°†é€šè¿‡APIç›´æ¥è®¿é—®DeepSeekçš„chatæ¨¡å‹ã€‚

```python
from openai import OpenAI
# ä»envæ–‡ä»¶ä¸­åŠ è½½api_key
from dotenv import load_dotenv
import os
load_dotenv()
# print(os.getenv("deepseek_api_key"))
client = OpenAI(
    api_key=os.getenv("deepseek_api_key"),
    base_url="https://api.deepseek.com"
)

response = client.chat.completions.create(
    model="deepseek-chat",
    messages=[
        {"role": "system", "content": "who are you "},
        {"role": "user", "content": "Hello"},
    ],
    stream=False
)

print('æ¨¡å‹åˆ—è¡¨ï¼š' + str(client.models.list().data))

print('å¼€åœºç™½ï¼š' + response.choices[0].message.content)
```

é€šè¿‡è°ƒè¯•ï¼Œå¾—åˆ°æ¨¡å‹åˆ—è¡¨ä¸ºï¼š

```sh
æ¨¡å‹åˆ—è¡¨ï¼š[Model(id='deepseek-chat', created=None, object='model', owned_by='deepseek'), Model(id='deepseek-reasoner', created=None, object='model', owned_by='deepseek')]
```

ç¨‹åºè¿”å›çš„å¼€åœºç™½ä¸ºï¼š

```sh
å¼€åœºç™½ï¼šHello! I'm an AI assistant here to help you with questions, information, or just to chat. How can I assist you today? ğŸ˜Š
```

å¯ä»¥çœ‹åˆ°ï¼Œå·²ç»å¯ä»¥æ­£å¸¸çš„è®¿é—®DeepSeekå¤§æ¨¡å‹å•¦ã€‚æ¥ä¸‹æ¥å°†è¿›ä¸€æ­¥å°è¯•å…¶ä»–çš„è°ƒç”¨æ–¹å¼ã€‚

## function calling æ¨¡å¼

## Jupyter Notebook è°ƒè¯•


